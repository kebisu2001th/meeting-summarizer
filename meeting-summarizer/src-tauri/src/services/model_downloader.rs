use crate::errors::AppResult;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tokio::time::Duration;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DownloadableModel {
    pub id: String,
    pub name: String,
    pub description: String,
    pub provider: String,
    pub file_size: Option<u64>,
    pub download_command: String,
    pub requirements: ModelRequirements,
    pub tags: Vec<String>,
    pub popularity: u32, // „ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÊï∞„Å™„Å©„ÅÆÊåáÊ®ô
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelRequirements {
    pub min_memory_mb: u64,
    pub recommended_memory_mb: u64,
    pub disk_space_mb: u64,
    pub gpu_required: bool,
    pub supported_platforms: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DownloadProgress {
    pub model_id: String,
    pub status: DownloadStatus,
    pub progress_percent: f32,
    pub downloaded_bytes: u64,
    pub total_bytes: Option<u64>,
    pub speed_bps: Option<u64>, // bytes per second
    pub eta_seconds: Option<u64>,
    pub error_message: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum DownloadStatus {
    Pending,
    Downloading,
    Installing,
    Completed,
    Failed,
    Cancelled,
}

pub struct ModelDownloader {
    client: Client,
    model_catalog: HashMap<String, DownloadableModel>,
}

impl ModelDownloader {
    pub fn new() -> Self {
        let client = Client::builder()
            .timeout(Duration::from_secs(300)) // 5ÂàÜ„ÅÆ„Çø„Ç§„É†„Ç¢„Ç¶„Éà
            .build()
            .expect("Failed to create HTTP client");

        let mut downloader = Self {
            client,
            model_catalog: HashMap::new(),
        };
        
        downloader.initialize_catalog();
        downloader
    }

    /// „É¢„Éá„É´„Ç´„Çø„É≠„Ç∞„ÅÆÂàùÊúüÂåñ
    fn initialize_catalog(&mut self) {
        let models = vec![
            // Ollama „É¢„Éá„É´
            DownloadableModel {
                id: "ollama:llama3.2:1b".to_string(),
                name: "Llama 3.2 1B".to_string(),
                description: "ËªΩÈáè„ÅßÈ´òÈÄü„Å™Ê±éÁî®Ë®ÄË™û„É¢„Éá„É´".to_string(),
                provider: "Ollama".to_string(),
                file_size: Some(1_300_000_000), // Á¥Ñ1.3GB
                download_command: "ollama pull llama3.2:1b".to_string(),
                requirements: ModelRequirements {
                    min_memory_mb: 2048,
                    recommended_memory_mb: 4096,
                    disk_space_mb: 1500,
                    gpu_required: false,
                    supported_platforms: vec!["windows".to_string(), "macos".to_string(), "linux".to_string()],
                },
                tags: vec!["Ê±éÁî®".to_string(), "ËªΩÈáè".to_string(), "È´òÈÄü".to_string()],
                popularity: 95,
            },
            DownloadableModel {
                id: "ollama:llama3.2:3b".to_string(),
                name: "Llama 3.2 3B".to_string(),
                description: "„Éê„É©„É≥„Çπ„ÅÆÂèñ„Çå„ÅüÊ±éÁî®Ë®ÄË™û„É¢„Éá„É´".to_string(),
                provider: "Ollama".to_string(),
                file_size: Some(2_000_000_000), // Á¥Ñ2GB
                download_command: "ollama pull llama3.2:3b".to_string(),
                requirements: ModelRequirements {
                    min_memory_mb: 4096,
                    recommended_memory_mb: 6144,
                    disk_space_mb: 2500,
                    gpu_required: false,
                    supported_platforms: vec!["windows".to_string(), "macos".to_string(), "linux".to_string()],
                },
                tags: vec!["Ê±éÁî®".to_string(), "„Éê„É©„É≥„Çπ".to_string(), "Êé®Â•®".to_string()],
                popularity: 90,
            },
            DownloadableModel {
                id: "ollama:llama3.2:7b".to_string(),
                name: "Llama 3.2 7B".to_string(),
                description: "È´òÂìÅË≥™„Å™Âá∫Âäõ„ÇíÁîüÊàê„Åô„ÇãÊ±éÁî®Ë®ÄË™û„É¢„Éá„É´".to_string(),
                provider: "Ollama".to_string(),
                file_size: Some(4_100_000_000), // Á¥Ñ4.1GB
                download_command: "ollama pull llama3.2:7b".to_string(),
                requirements: ModelRequirements {
                    min_memory_mb: 8192,
                    recommended_memory_mb: 16384,
                    disk_space_mb: 5000,
                    gpu_required: false,
                    supported_platforms: vec!["windows".to_string(), "macos".to_string(), "linux".to_string()],
                },
                tags: vec!["Ê±éÁî®".to_string(), "È´òÂìÅË≥™".to_string()],
                popularity: 85,
            },
            DownloadableModel {
                id: "ollama:mistral:7b".to_string(),
                name: "Mistral 7B".to_string(),
                description: "ÂäπÁéáÁöÑ„ÅßÂ§öË®ÄË™ûÂØæÂøú„ÅÆË®ÄË™û„É¢„Éá„É´".to_string(),
                provider: "Ollama".to_string(),
                file_size: Some(4_200_000_000), // Á¥Ñ4.2GB
                download_command: "ollama pull mistral:7b".to_string(),
                requirements: ModelRequirements {
                    min_memory_mb: 8192,
                    recommended_memory_mb: 16384,
                    disk_space_mb: 5200,
                    gpu_required: false,
                    supported_platforms: vec!["windows".to_string(), "macos".to_string(), "linux".to_string()],
                },
                tags: vec!["Â§öË®ÄË™û".to_string(), "ÂäπÁéáÁöÑ".to_string()],
                popularity: 80,
            },
            DownloadableModel {
                id: "ollama:codellama:7b".to_string(),
                name: "Code Llama 7B".to_string(),
                description: "„Ç≥„Éº„ÉâÁîüÊàê„Å´ÁâπÂåñ„Åó„ÅüË®ÄË™û„É¢„Éá„É´".to_string(),
                provider: "Ollama".to_string(),
                file_size: Some(3_800_000_000), // Á¥Ñ3.8GB
                download_command: "ollama pull codellama:7b".to_string(),
                requirements: ModelRequirements {
                    min_memory_mb: 8192,
                    recommended_memory_mb: 16384,
                    disk_space_mb: 4500,
                    gpu_required: false,
                    supported_platforms: vec!["windows".to_string(), "macos".to_string(), "linux".to_string()],
                },
                tags: vec!["„Ç≥„Éº„ÉâÁîüÊàê".to_string(), "„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞".to_string()],
                popularity: 75,
            },
        ];

        for model in models {
            self.model_catalog.insert(model.id.clone(), model);
        }
    }

    /// Âà©Áî®ÂèØËÉΩ„Å™„É¢„Éá„É´‰∏ÄË¶ß„ÇíÂèñÂæó
    pub fn get_downloadable_models(&self) -> Vec<&DownloadableModel> {
        let mut models: Vec<&DownloadableModel> = self.model_catalog.values().collect();
        models.sort_by(|a, b| b.popularity.cmp(&a.popularity));
        models
    }

    /// „Ç´„ÉÜ„Ç¥„É™Âà•„É¢„Éá„É´‰∏ÄË¶ß„ÇíÂèñÂæó
    pub fn get_models_by_category(&self, category: &str) -> Vec<&DownloadableModel> {
        self.model_catalog
            .values()
            .filter(|model| {
                match category {
                    "lightweight" => model.requirements.min_memory_mb <= 4096,
                    "balanced" => model.requirements.min_memory_mb > 4096 && model.requirements.min_memory_mb <= 8192,
                    "high-quality" => model.requirements.min_memory_mb > 8192,
                    "code" => model.tags.contains(&"„Ç≥„Éº„ÉâÁîüÊàê".to_string()),
                    "multilingual" => model.tags.contains(&"Â§öË®ÄË™û".to_string()),
                    _ => true,
                }
            })
            .collect()
    }

    /// „Ç∑„Çπ„ÉÜ„É†Ë¶Å‰ª∂„ÉÅ„Çß„ÉÉ„ÇØ
    pub fn check_system_requirements(&self, model_id: &str) -> Result<SystemCompatibility, String> {
        let model = self.model_catalog.get(model_id)
            .ok_or_else(|| format!("Model not found: {}", model_id))?;

        let available_memory = self.get_available_memory();
        let available_disk = self.get_available_disk_space();
        let platform = self.get_current_platform();

        let memory_ok = available_memory >= model.requirements.min_memory_mb;
        let disk_ok = available_disk >= model.requirements.disk_space_mb;
        let platform_ok = model.requirements.supported_platforms.contains(&platform);

        let compatibility = SystemCompatibility {
            model_id: model_id.to_string(),
            memory_compatible: memory_ok,
            disk_compatible: disk_ok,
            platform_compatible: platform_ok,
            available_memory_mb: available_memory,
            required_memory_mb: model.requirements.min_memory_mb,
            available_disk_mb: available_disk,
            required_disk_mb: model.requirements.disk_space_mb,
            warnings: self.generate_compatibility_warnings(model, available_memory, available_disk),
        };

        Ok(compatibility)
    }

    /// Ollama„É¢„Éá„É´„ÅÆ„ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÈñãÂßã
    pub async fn start_download_ollama(&self, model_name: &str) -> AppResult<DownloadProgress> {
        log::info!("üì• Starting Ollama model download: {}", model_name);
        
        // Ollama„ÅåÂà©Áî®ÂèØËÉΩ„Åã„ÉÅ„Çß„ÉÉ„ÇØ
        self.check_ollama_availability().await?;
        
        // pull„Ç≥„Éû„É≥„Éâ„ÇíÂÆüË°åÔºàÂÆüÈöõ„ÅÆÂÆüË£Ö„Åß„ÅØÈùûÂêåÊúü„Éó„É≠„Çª„ÇπÂÆüË°åÔºâ
        let progress = DownloadProgress {
            model_id: format!("ollama:{}", model_name),
            status: DownloadStatus::Downloading,
            progress_percent: 0.0,
            downloaded_bytes: 0,
            total_bytes: self.model_catalog.get(&format!("ollama:{}", model_name))
                .and_then(|m| m.file_size),
            speed_bps: None,
            eta_seconds: None,
            error_message: None,
        };
        
        // ÂÆüÈöõ„ÅÆÂÆüË£Ö„Åß„ÅØ„ÄÅ„Åì„Åì„Åß„Ç≥„Éû„É≥„Éâ„ÇíÈùûÂêåÊúüÂÆüË°å„Åó„ÄÅÈÄ≤Êçó„ÇíËøΩË∑°
        log::info!("üîÑ Would execute: ollama pull {}", model_name);
        
        Ok(progress)
    }

    /// GPT4All„É¢„Éá„É´„ÅÆ„ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÊÉÖÂ†±ÂèñÂæó
    pub fn get_gpt4all_download_info(&self, model_name: &str) -> Result<String, String> {
        let download_url = match model_name {
            "orca-mini-3b" => "https://gpt4all.io/models/orca-mini-3b-gguf2-q4_0.gguf",
            "vicuna-7b" => "https://gpt4all.io/models/vicuna-7b-q4_0.gguf",
            "falcon-7b" => "https://gpt4all.io/models/falcon-7b-q4_0.gguf",
            _ => return Err(format!("Unknown GPT4All model: {}", model_name)),
        };
        
        Ok(download_url.to_string())
    }

    /// „Ç∑„Çπ„ÉÜ„É†‰∫íÊèõÊÄß„ÉÅ„Çß„ÉÉ„ÇØÁî®„ÅÆ„Éò„É´„Éë„ÉºÈñ¢Êï∞
    fn get_available_memory(&self) -> u64 {
        // ÂÆüÈöõ„ÅÆÂÆüË£Ö„Åß„ÅØ„Ç∑„Çπ„ÉÜ„É†„É°„Éà„É™„ÇØ„Çπ„Çí‰ΩøÁî®
        16384 // 16GB „Å®‰ªÆÂÆö
    }

    fn get_available_disk_space(&self) -> u64 {
        // ÂÆüÈöõ„ÅÆÂÆüË£Ö„Åß„ÅØ„Éá„Ç£„Çπ„ÇØ‰ΩøÁî®Èáè„ÇíÂèñÂæó
        100_000 // 100GB „Å®‰ªÆÂÆö
    }

    fn get_current_platform(&self) -> String {
        #[cfg(target_os = "windows")]
        return "windows".to_string();
        
        #[cfg(target_os = "macos")]
        return "macos".to_string();
        
        #[cfg(target_os = "linux")]
        return "linux".to_string();
        
        #[cfg(not(any(target_os = "windows", target_os = "macos", target_os = "linux")))]
        return "unknown".to_string();
    }

    fn generate_compatibility_warnings(&self, model: &DownloadableModel, available_memory: u64, available_disk: u64) -> Vec<String> {
        let mut warnings = Vec::new();
        
        if available_memory < model.requirements.recommended_memory_mb {
            warnings.push(format!(
                "Êé®Â•®„É°„É¢„É™ÂÆπÈáè {}MB „Å´ÂØæ„Åó„Å¶Âà©Áî®ÂèØËÉΩ„É°„É¢„É™„Åå {}MB „Åß„Åô„ÄÇ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Åå‰Ωé‰∏ã„Åô„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ",
                model.requirements.recommended_memory_mb,
                available_memory
            ));
        }
        
        if available_disk < model.requirements.disk_space_mb * 2 {
            warnings.push(format!(
                "„Éá„Ç£„Çπ„ÇØÂÆπÈáè„Å´‰ΩôË£ï„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ{}MB ‰ª•‰∏ä„ÅÆÁ©∫„ÅçÂÆπÈáè„ÇíÁ¢∫‰øù„Åô„Çã„Åì„Å®„ÇíÊé®Â•®„Åó„Åæ„Åô„ÄÇ",
                model.requirements.disk_space_mb * 2
            ));
        }
        
        if model.requirements.gpu_required {
            warnings.push("„Åì„ÅÆ„É¢„Éá„É´„ÅØGPU„Ç¢„ÇØ„Çª„É©„É¨„Éº„Ç∑„Éß„É≥„ÇíÊé®Â•®„Åó„Åæ„Åô„ÄÇ".to_string());
        }
        
        warnings
    }

    async fn check_ollama_availability(&self) -> AppResult<()> {
        match self.client.get("http://localhost:11434/api/version").send().await {
            Ok(response) if response.status().is_success() => Ok(()),
            _ => Err(crate::errors::AppError::LLMConnectionError {
                message: "Ollama is not running. Please start Ollama first.".to_string(),
            }),
        }
    }

    /// „É¢„Éá„É´Ê§úÁ¥¢Ê©üËÉΩ
    pub fn search_models(&self, query: &str, tags: &[String]) -> Vec<&DownloadableModel> {
        let query_lower = query.to_lowercase();
        
        self.model_catalog
            .values()
            .filter(|model| {
                // „ÉÜ„Ç≠„Çπ„ÉàÊ§úÁ¥¢
                let text_match = model.name.to_lowercase().contains(&query_lower)
                    || model.description.to_lowercase().contains(&query_lower);
                
                // „Çø„Ç∞Ê§úÁ¥¢
                let tag_match = tags.is_empty() || tags.iter().any(|tag| model.tags.contains(tag));
                
                text_match && tag_match
            })
            .collect()
    }

    /// ‰∫∫Ê∞ó„É¢„Éá„É´„ÅÆÊé®Â•®
    pub fn get_popular_models(&self, limit: usize) -> Vec<&DownloadableModel> {
        let mut models: Vec<&DownloadableModel> = self.model_catalog.values().collect();
        models.sort_by(|a, b| b.popularity.cmp(&a.popularity));
        models.into_iter().take(limit).collect()
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SystemCompatibility {
    pub model_id: String,
    pub memory_compatible: bool,
    pub disk_compatible: bool,
    pub platform_compatible: bool,
    pub available_memory_mb: u64,
    pub required_memory_mb: u64,
    pub available_disk_mb: u64,
    pub required_disk_mb: u64,
    pub warnings: Vec<String>,
}

impl SystemCompatibility {
    pub fn is_fully_compatible(&self) -> bool {
        self.memory_compatible && self.disk_compatible && self.platform_compatible
    }
}

impl Default for ModelDownloader {
    fn default() -> Self {
        Self::new()
    }
}